---
title: "Supplementary Material (SM)"
bibliography: ref.bib
output:
  pdf_document: 
    extra_dependencies:
      - float
      - flafter
    fig_caption: yes
    fig_height: 3
    fig_width: 4
    keep_tex: yes
    number_sections: yes
header-includes:
  - \usepackage{xr}
  - \usepackage{mathtools}
  - \usepackage{mathrsfs}
  - \usepackage{hyperref}
  - \usepackage[left]{lineno}
  - \usepackage{amsmath}
  - \linenumbers
  - \externaldocument{draft}  
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
```

Throughout this supplement, we set use dot notation for time derivatives so that $\dot f(x,t)=\frac{\partial }{\partial t}f(x,t)$ and set $\Delta=\sum_{i=1}^d\frac{\partial^2}{\partial x_i^2}$ the Laplace operator on $\mathbb{R}^d$, except in \S\ref{caC} where $\Delta$ represents a random variable.

<!--

## Existence, uniqueness and regularity of solution to deterministic PDE (\ref{eq1}) of the main text \label{regular}

Here we show that solutions to the PDE described in the main text exist, are unique and are twice continuously differentiable with respect to space. Our approach is an adaptation of that taken in chapter 9 \S2.2 of Volpert [-@Volpert2014].

Recall that the PDE we consider is of the form
\begin{equation}\label{PDE_supp}
\begin{matrix}
\dot\nu(x,t)=m(\nu,x)\nu(x,t)+\frac{\mu}{2}\Delta\nu(x,t), \\
\\
\nu(x,0)=u(x),
\end{matrix}
\end{equation}
where $m(\nu,x)\leq r\in\mathbb{R}$ is shorthand for $m((K\nu)(x,t),x)$ and $(K\nu)(x,t)=\int_\mathbb{R}\kappa(x-y)\nu(y,t)dy$ for some non-negative, bounded $\kappa$. Denoting $C(\mathbb{R})$ the set of continuous functions on $\mathbb{R}$ and $L^1(\mathbb{R})$ the Lebesgue space of integrable functions on $\mathbb{R}$ (ie, the set of functions $f:\mathbb{R}\to\mathbb{R}$ that satisfy $\|f\|_1=\int_\mathbb{R}|f(x)|dx<+\infty$ under the equivalence relation $f=g$ iff $\|f-g\|_1=0$), we assume a continuous and integrable initial condition $\nu(\cdot,0)=u\in L^1(\mathbb{R})\cap C(\mathbb{R})$. We now prove the existence and uniqueness of solutions to this Cauchy problem.

Set 
\begin{equation}
\Phi(x,t)=\frac{\exp\left(-x^2/2\mu t\right)}{\sqrt{2\pi\mu t}}.
\end{equation}
Then the semigroup $\{S(t)\}_{t\geq0}$ given by 
\begin{equation}
S(t)v=\left\{\begin{matrix}
\int_0^t\frac{\mu}{2}\Delta\nu(x,s)ds=\int_\mathbb{R}\Phi(x-y,t)v(y)dy, & t>0 \\
 & \\
v, &t=0
\end{matrix}\right.
\end{equation}
for all $v\in L^\infty(\mathbb{R})$ satisfies $\|S(t)v\|_1=\|v\|_1$ for all $t\geq0$ and $v\in L^1(\mathbb{R})$. We also define the operator
\begin{equation}
(Fv)(x)=m\left(\int_\mathbb{R}\kappa(x-y)v(y)dy,x\right)v(x), \ \ \forall \ \ v\in L^1(\mathbb{R}).
\end{equation}
Thus $\|Fv\|_1\leq |r| \ \|v\|_1$ for all $v\in L^1(\mathbb{R})$. Then, for any $v,w\in L^1(\mathbb{R})$,
\begin{multline}
Fv-Fw=F(v+w)-F(v+w)+Fv-Fw \\
=m(v+w,x)(v+w)-m(w,x)w-m(v+w,x)(v+w)+m(v,x)v \\
=\Big(m(v+w,x)-m(w,x)\Big)w-\Big(m(v+w,x)-m(v,x)\Big)v+m(v+w,x)(v-w).
\end{multline}
Hence $\|Fv-Fw\|_1\leq2|r|(\|w\|_1+\|v\|_2)+|r|\|v-w\|_1\leq\|r\|\|v-w\|_1$ which implies $F$ is Lipschitz continuous on $L^1(\mathbb{R})$. For $T>0$ set $L^{\infty,1}([0,T]\times\mathbb{R})$ as the set of functions $\nu:[0,T]\to L^1(\mathbb{R})$ that satisfy $\|\nu\|_{1,T}=\sup_{t\in[0,T]}\|\nu(t)\|_1<+\infty$ and consider the operator $G$ defined on $L^{\infty,1}([0,T]\times\mathbb{R})$ for fixed $u\in L^1(\mathbb{R})$ by
\begin{equation}
(Gv)(t)=S(t)u+\int_0^tS(t-s)(Fv(s))ds \ \ \forall \ \ t\in[0,T].
\end{equation}
Then for any $t\in[0,T]$ and $\nu_1,\nu_2\in L^{\infty,1}([0,T]\times\mathbb{R})$,

\begin{multline}
\|G\nu_1-G\nu_2\|_{1,T}=\sup_{t\in[0,T]}\left\|\int_0^tS(t-s)(F\nu_1(s)-F\nu_2(s))ds\right\|_1 \\ \leq \sup_{t\in[0,T]}t|r|\|\nu_1(t)-\nu_2(t)\|_1\leq T|r|\|\nu_1-\nu_2\|_{1,T}.
\end{multline}
Hence, when $T<\frac{1}{|r|}$ the operator $G$ is a contraction operator. In this case we apply Banach's fixed point theorem to demonstrate $G$ has a unique fixed point. That is, there exists a unique $\nu\in L^{\infty,1}([0,T]\times\mathbb{R})$ such that $G\nu=\nu$. Then, for $\nu=\nu(x,t)$ and considering $\nu(t)$ the function $x\to\nu(x,t)$, we have by definition $\nu$ is the mild solution of PDE (\ref{PDE_supp}) with initial condition $\nu(0)=u$. To extend this to arbitrarily large $T$ we take a similar approach as Evans [-@lawrenceevans2010] and choose $T_1<\frac{1}{|r|}$ and apply the above argument to the interval $[T_1,2T_1]$. Repeating this task a finite number of times shows $\nu$ is the mild solution of PDE (\ref{PDE_supp}) with initial condition $\nu(0)=u$ for arbitrarily large $T>0$. 

We shift our focus to showing mild solutions $\nu$ to PDE (\ref{PDE_supp}) are twice differentiable in $x$ and hence are classical solutions. Applying lemma 3.3.2 of Henry [-@Henry1981] we have that the mild solution $\nu$ is in fact a strong solution (i.e., differentiable with respect to $t\in(0,T)$) to the abstract Cauchy problem defined by PDE (\ref{PDE_supp}) with initial condition $\nu(0)=u$ for $t\in[0,T]$. In particular, this implies that $\nu(t)\in D(\Delta)$ for each $t\in(0,T)$ where $D(\Delta)=W^{2,1}(\mathbb{R})$ is the Sobolov space of integrable functions with integrable weak derivatives at least of order $2$. Since $\frac{1}{p}\geq\frac{1}{p+1}\geq\frac{1}{p}-2$ for all $p\geq1$, the Sobolov embedding theorem [@Henry1981] implies $D(\Delta)\subset L^p(\mathbb{R})$, the Lebesgue space of $p$-integrable functions on $\mathbb{R}$, for all $p\geq1$. Hence $D(\Delta)\subset W^{2,p}(\mathbb{R})$, the Sobolov space of $p$-integrable functions with weak derivatives at least of order $2$, for all $p\geq1$. 

If $v\in C^{2+\delta}(\mathbb{R})$ for some $\delta>0$, then 
\begin{equation}
(Kv)(\cdot)=\int_\mathbb{R}\kappa(\cdot-y)v(y)dy=\int_\mathbb{R}\kappa(y)v(\cdot-y)dy\in C^{2+\delta}(\mathbb{R}),
\end{equation}
and, since we assume $m(h,x)$ is continuous in both arguments, $Fv=m(Kv,\cdot)v\in C^\delta$.


Why is $F\nu\in C^\delta(\mathbb{R})$ for some $\delta>0$? Why does this imply $\nu(\cdot,t)\in C^{2+\delta}(\mathbb{R})$ for all $t\in(0,T)$? This argument holds for all $T>0$. Hence, solutions to the Cauchy problem (\ref{PDE_supp}) are twice continuously differentiable in their spatial parameter for all $t>0$.

-->

# Sufficient conditions for finite mean, variance and total abundance in the deterministic case \label{finite}

Recall that $m(\nu,x)$ is shorthand for $m\big((K\nu)(x,t),x\big)$. That is, $m:[0,+\infty)\times\mathbb{R}\to\mathbb{R}$. Following our assumptions of the main text, we have that $m(h,x)$ is differentiable with respect to both $x$ and $h$ and there exists $r\in\mathbb{R}$ such that $m(h,x)\leq r$ across all $h\geq0$ and $x\in\mathbb{R}$. As in the main text, we also assume the initial condition $u(x)$ is continuous and integrable in $x$ and satisfies
\begin{equation}
0\leq\int_{\mathbb{R}}(|x|+x^2)u(x)dx<+\infty
\end{equation}
and consider the Cauchy problem
\begin{equation}\label{PDE_SM}
\left\{\begin{matrix}
\dot\nu(x,t)=m(\nu,x)\nu(x,t)+\frac{\mu}{2}\Delta\nu(x,t) & t>0\\
\nu(x,0)=u(x) & t=0.
\end{matrix}\right.
\end{equation}

According to Theorem 2.5.6 of Zheng [-@zheng2004nonlinear], if the operator $F$ defined by $\nu(x,t)\to m(\nu,x)\nu(x,t)$ is locally Lipschitz, corresponding to equation (\ref{local_lipschitz}) of the main text, then for some maximal $T>0$ problem (\ref{PDE_SM}) admits a unique local classical solution $\nu(x,t)$ for $t\in[0,T)$. Furthermore, either $T=+\infty$ or $T<+\infty$ and $\lim_{t\uparrow T}\int_\mathbb{R}|\nu(x,t)|dx=+\infty$. In this section we show that our assumption $m(h,x)\leq r$ for all $h\geq0$ and $x\in\mathbb{R}$ implies $T=+\infty$. Replacing $m$ with it's upper bound $r\in\mathbb{R}$, equation (\ref{PDE_SM}) reduces to a simple parabolic equation that can be solved using elementary techniques [@stanleyfarlow1993]. In particular, when $m(\nu,x)\equiv0$ denote the solution to (\ref{PDE_SM}) by $\nu_0(x,t)$. Then, denoting
\begin{equation}
\Phi(x,t)=\frac{\exp\left(-x^2/2\mu t\right)}{\sqrt{2\pi\mu t}},
\end{equation}
we have
\begin{equation}
\nu_0(x,t)=\int_{\mathbb{R}}\Phi(x-y,t)\nu(y,0)dy.
\end{equation}
In the more general case, when $m(\nu,x)\equiv r\in\mathbb{R}$, equation (\ref{PDE_SM}) has the solution $\nu_r(x,t)=e^{rt}\nu_0(x,t)$. Hence, $\nu_r(x,t)\geq0$ for all $x\in\mathbb{R}$ and $\int_\mathbb{R}\nu_r(x,t)dx=e^{rt}N(0)<+\infty$ for all $t\geq0$. Furthermore, denoting 
\begin{equation}
\begin{matrix}
N_r(t)&=&\int_\mathbb{R}\nu_r(x,t)dx, \\
p_r(x,t)&=&\nu_r(x,t)/N_r(t), \\
\bar x_r(t)&=&\int_\mathbb{R}xp_r(x,t)dx, \\
\sigma^2_r(t)&=&\int_\mathbb{R}(x-\bar x_r(t))^2p_r(x,t)dx, 
\end{matrix}
\end{equation}
we have
\begin{equation}
\bar x_r(t)=\int_\mathbb{R}x\int_\mathbb{R}\Phi(x-y,t)p(y,0)dydx=\int_\mathbb{R}yp(y,0)dy=\bar x(0),
\end{equation}
\begin{equation}
\sigma_r^2(t)=\int_\mathbb{R}(x-\bar x_r(t))^2\int_\mathbb{R}\Phi(x-y,t)p(y,0)dydx=\int_\mathbb{R}\Big((y-\bar x(0))^2+\mu t\Big)p(y,0)dy=\sigma^2(0)+\mu t.
\end{equation}
Hence, $|\bar x_r(t)|,\sigma_r^2(t)<+\infty$ for all $t\geq0$. For the sake of contradiction, suppose there exists $x\in\mathbb{R}$ and $t\geq0$ such that $\nu(x,t)>\nu_r(x,t)$. Then
\begin{equation}
\nu(x,t)-\nu(x,0)=\int_0^tm(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)ds>\int_0^tr\nu_r(x,s)+\frac{\mu}{2}\Delta\nu_r(x,s)ds=\nu_r(x,t)-\nu(x,0)
\end{equation}
which implies there exists $h\geq0$ and $x\in\mathbb{R}$ such that $m(h,x)>r$. But this contradicts our assumption $m(h,x)\leq r$ for all $h\geq0$ and $x\in\mathbb{R}$. So we have $\nu(x,t)\leq\nu_r(x,t)$ for each $x\in\mathbb{R}$ and $t\geq0$. This implies that $N(t)=\int_\mathbb{R}\nu(x,t)dx<+\infty$,
\begin{equation}
0\leq\int_\mathbb{R}x^2\nu(x,t)dx\leq\int_\mathbb{R}x^2\nu_r(x,t)dx<+\infty
\end{equation}
and in particular
\begin{equation}
0\leq\sigma^2(t)+\bar x^2(t)=\frac{1}{N(t)}\int_\mathbb{R}x^2\nu(x,t)dx<+\infty
\end{equation}
for each $t\geq0$.

# Equilibrium moments for a deterministic population experiencing logistic growth and stabilizing selection \label{equilib}

Here we set out to show, given the initial conditions $\nu(\cdot,0)\in L^1(\mathbb{R})$ such that $\bar x(0)\in\mathbb{R}$, $\sigma^2(0)\in[0,+\infty)$ and $N(0)\in(0,+\infty)$, and given the growth rate
\begin{equation}\label{m_log_stab}
m(\nu,x)=r-\frac{a}{2}(\theta-x)^2-c\int_\mathbb{R}\nu(y,t)dy
\end{equation}
such that $\theta\in\mathbb{R}$, $a,c,\mu>0$ and $r>\tfrac{1}{2}\sqrt{\mu a}$, we have the following stable equilibrium $N_\infty=\lim_{t\to\infty}N(t)=\tfrac{1}{c}(r-\tfrac{1}{2}\sqrt{a\mu})$, $\bar x_\infty=\lim_{t\to\infty}\bar x(t)=\theta$ and $\sigma^2_\infty=\lim_{t\to\infty}\sigma^2(t)=\sqrt{\tfrac{\mu}{a}}$. 

The mean fitness can be calculated as
\begin{equation}
\bar m(t)=r-\frac{a}{2}((\theta-\bar x(t))^2+\sigma^2(t))-cN(t).
\end{equation}
Recalling the ODE derived for $N(t)$,
\begin{equation}
\frac{d}{dt}N(t)=\bar m(t)N(t)=(r-\frac{a}{2}((\theta-\bar x(t))^2+\sigma^2(t))-cN(t))N(t),
\end{equation}
solving for equilibrium total abundance $\hat N$ amounts to setting $\frac{d}{dt}N(t)=0$ and solving for $N(t)$. Ignoring the equilibrium $N(t)=0$, this reduces to solving $\bar m(t)=0$ for $N(t)$, which returns
\begin{equation}
\hat N=\frac{1}{c}\left(r-\frac{a}{2}\left((\theta-\hat{\bar x})^2-\hat\sigma^2\right)\right).
\end{equation}
Unfortunately, deriving ODE for $\bar x(t)$ and $\sigma^2(t)$ leads to expressions involving higher moments and finding ODE for these higher moments will lead to expressions involving yet even higher moments. To avoid this infinite regression, we find the equilibrium abundance density $\hat\nu(x)$ by solving $\frac{\partial}{\partial t}\nu(x,t)=0$ for $\nu(x,t)$. This implies the following ordinary differential equation
\begin{equation}
\frac{d^2}{dx^2}\hat\nu(x)=\left(\frac{2c}{\mu}\hat N+\frac{a}{\mu}(\theta-x)^2-\frac{2r}{\mu}\right)\hat\nu(x)
\end{equation}
which has the solution
\begin{equation}
\hat\nu(x)=\frac{\hat N}{\sqrt{2\pi}}\left(\frac{a}{\mu}\right)^{\frac{1}{4}}\exp\left(-\sqrt\frac{a}{\mu}\frac{(\theta-x)^2}{2}\right).
\end{equation}
From this expression we infer $\hat{\bar x}=\theta$ and $\hat\sigma^2=\sqrt{\frac{\mu}{a}}$. Hence $\hat N=\frac{1}{c}\left(r-\frac{1}{2}\sqrt{a\mu}\right)$. To show that $N_\infty=\hat N$, $\bar x_\infty=\hat{\bar x}$ and $\sigma^2_\infty=\hat\sigma^2$, we linearize ODE for $N(t)$, $\bar x(t)$ and $\sigma^2(t)$ using the equilibrium $\hat\nu(x)$. In particular, since $\hat\nu(x)$ is Gaussian, we do not run into the same issue with higher moments as above. Assuming the equilibrium $\hat\nu(x)$ does not change the ODE for $N(t)$, but the ODE for $\bar x(t)$ and $\sigma^2(t)$ can now be expressed as
\begin{equation}
\frac{d}{dt}\bar x(t)=\sigma^2(t)\left(\frac{\partial\bar m(t)}{\partial\bar x(t)}-\overline{\frac{\partial m(t)}{\partial\bar x(t)}}\right)=a\sigma^2(t)(\theta-\bar x(t)),
\end{equation}
\begin{equation}
\frac{d}{dt}\sigma^2(t)=2\sigma^4(t)\left(\frac{\partial\bar m(t)}{\partial\sigma^2(t)}-\overline{\frac{\partial m(t)}{\partial\sigma^2(t)}}\right)+\mu=\mu-a\sigma^4(t),
\end{equation}
where $\hat p(x)=\hat\nu(x)/\hat N$. These expressions confirm our findings that $\hat{\bar x}=\theta$ and $\hat\sigma^2=\sqrt{\frac{\mu}{a}}$. Furthermore, calculating
\begin{equation}
\frac{\partial}{\partial\sigma^2(t)}\frac{d}{dt}\sigma^2(t)=-2a\sigma^2(t)
\end{equation}
and evaluating at $\sigma^2(t)=\hat\sigma^2$ demonstrates the equilibrium phenotypic variance is stable when $a,\mu>0$. Hence, calculating
\begin{equation}
\frac{\partial}{\partial\bar x(t)}\frac{d}{dt}\bar x(t)=-a\sigma^2(t)
\end{equation}
and evaluating at $\sigma^2(t)=\hat\sigma^2$ and $\bar x(t)=\hat{\bar x}$ demonstrates the equilibrium phenotypic mean is stable when $a,\mu>0$. Finally, calculating
\begin{equation}
\frac{\partial}{\partial N(t)}\frac{d}{dt}N(t)=r-\frac{a}{2}((\theta-\bar x(t))^2+\sigma^2(t))-2cN(t)
\end{equation}
and evaluating at $\sigma^2(t)=\hat\sigma^2$, $\bar x(t)=\hat{\bar x}$ and $N(t)=\hat N$ demonstrates the equilibrium total abundance is stable when $a,c,\mu>0$, and $r>\frac{1}{2}\sqrt{a\mu}$.

# The relation between diffusion and convolution with a Gaussian kernel \label{diffconvequiv}

For continuous $g:\mathbb{R}^d\to\mathbb{R}$, consider the deterministic Cauchy problem
\begin{equation}\label{heateqn}
\left\{\begin{matrix}
\dot f(x,t)=&\Delta f(x,t), & (x,t)\in\mathbb{R}^d\times(0,\infty)\\
f(x,t)=&g(x), & (x,t)\in\mathbb{R}^d\times\{0\}.
\end{matrix}\right.
\end{equation}
According to Evans [-@lawrenceevans2010], the fundamental solution of (\ref{heateqn}) is
\begin{equation}
\Phi(x,t)=\frac{1}{(4\pi t)^{d/2}}\exp\left(-\frac{|x|^2}{4t}\right), \ (x,t)\in(0,\infty)\times\mathbb{R}^d,
\end{equation}
where $|x|=\sqrt{\sum_ix_i^2}$. The solution $f(x,t)$ of PDE (\ref{heateqn}) is then given by the convolution
\begin{equation}
f(x,t)=\int_{\mathbb{R}^d}\Phi(x-y,t)g(y)dy, \ (x,t)\in(0,\infty)\times\mathbb{R}^d.
\end{equation}
Hence, by the fundamental theorem of calculus,

\begin{multline}
f(x,t)+\int_t^{t+1}\dot f(x,s)ds=f(x,t+1) \\
=\int_{\mathbb{R}^d}\Phi(x-y,t+1)g(y)dy=\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\Phi(x-y,1)\Phi(y-z,t)g(z)dzdy \\
=\int_{\mathbb{R}^d}\Phi(x-y,1)f(t,y)dy.
\end{multline}
In particular,
\begin{equation}
f(x,t)+\int_t^{t+1}\Delta f(x,s)ds=\int_{\mathbb{R}^d}\Phi(1,x-y)f(y,t)dy.
\end{equation}

# Deterministic dynamics of $\sigma^2(t)$ \label{var_deriv}

Picking up from the main text \S\ref{deterministic},

\begin{multline}
\frac{d}{dt}\sigma^2(t)=\frac{d}{dt}\int_\mathbb{R}(x-\bar x(t))^2p(x,t)dx=\int_\mathbb{R}2(x-\bar x(t))\frac{d}{dt}{\bar x}(t)+(x-\bar x(t))^2\frac{\partial}{\partial t} p(x,t)dx\\=\int_\mathbb{R}(x-\bar x(t))^2\left((m(\nu,x)-\bar m(t))p(x,t)+\frac{\mu}{2}\frac{\partial^2}{\partial x^2}p(x,t)\right)dx\\=\int_\mathbb{R}\left((x-\bar x(t))^2-\sigma^2(t)+\sigma^2(t)\right)(m(\nu,x)-\bar m(t))p(x,t)+(x-\bar x(t))^2\frac{\mu}{2}\frac{\partial^2}{\partial x^2}p(x,t)dx\\=\mathrm{Cov}_t\Big((x-\bar x(t))^2,m(\nu,x)\Big)+\frac{\mu}{2}\int_\mathbb{R}(x-\bar x(t))^2\frac{\partial^2}{\partial x^2}p(x,t)dx.
\end{multline}
Applying integration by parts twice yields
\begin{equation}
\int_{-\infty}^{+\infty}(x-\bar x(t))^2\frac{\partial^2}{\partial x^2}p(x,t)dx=2.
\end{equation}

## Simplifying covariances with fitness under the assumption of a Gaussian density \label{cov2deriv}

By assuming

\begin{equation}
p(x,t)=\frac{\exp\left(-\frac{(x-\bar x(t))^2}{2\sigma^2(t)}\right)}{\sqrt{2\pi\sigma^2(t)}}
\end{equation}

we have

\begin{multline}
\sigma^2\left(\frac{\partial\bar m}{\partial\bar x}-\overline{\frac{\partial m}{\partial\bar x}}\right)=\sigma^2\left(\frac{\partial}{\partial\bar x}\int_\mathbb{R}m(\nu,x)p(x,t)dx-\int_\mathbb{R}p(x,t)\frac{\partial}{\partial \bar x}m(\nu,x)dx\right)\\=\sigma^2\int_\mathbb{R}m(\nu,x)\frac{\partial}{\partial\bar x}p(x,t)dx=\sigma^2\int_\mathbb{R}\frac{x-\bar x(t)}{\sigma^2}m(\nu,x)p(x,t)dx\\=\int_\mathbb{R}(x-\bar x)(m(\nu,x)-\bar m)p(x,t)dx=\mathrm{Cov}_t(m,x),
\end{multline}

and

\begin{multline}
2\sigma^4\left(\frac{\partial\bar m}{\partial\sigma^2}-\overline{\frac{\partial m}{\partial\sigma^2}}\right)=2\sigma^4\left(\frac{\partial}{\partial\sigma^2}\int_\mathbb{R}m(\nu,x)p(x,t)dx-\int_\mathbb{R}p(x,t)\frac{\partial}{\partial\sigma^2}m(\nu,x)dx\right)\\=2\sigma^4\int_\mathbb{R}\frac{(x-\bar x)^2-\sigma^2}{2\sigma^4}m(\nu,x)p(x,t)dx=\int_\mathbb{R}\left((x-\bar x)^2-\sigma^2\right)\big(m(\nu,x)-\bar m\big)p(x,t)dx\\=\mathrm{Cov}_t\Big((x-\bar x)^2,m\Big).
\end{multline}

# Comparing our treatment of white noise to Da Prato and Zabczyk (2014) \label{DPZ14}

Our approach in the main text is inspired by the treatment provided in \S4.2 of Da Prato and Zabczyk [-@DaPrato2014]. Here the authors develop a stochastic integral of operator-valued processes. In particular, they consider processes indexed by time $t\geq0$ valued as Hilbert-Schmidt operators $\Phi(t)$ and define the norm
\begin{equation}
\|\Phi\|_t=\sqrt{\mathbb{E}\left(\int_0^t\mathrm{Tr}[\Phi(s)\Phi^*(s)]ds\right)}, \ t\geq0.
\end{equation}
In our case we only consider the so-called multiplication operators. That is, processes that consist of operators $\Phi(t)$ having the form $\Phi(t)g(x)=\varphi(x,t)g(x)$ such that $\varphi(\cdot,t)\in L^2(\mathbb{R})$ a.s. for each $t\geq0$. In this case $\Phi(t)=\Phi^*(t)$ and
\begin{equation}
\|\Phi\|_t=\|\varphi\|_t=\sqrt{\mathbb{E}\left(\int_0^t\int_\mathbb{R}\varphi^2(x,s)dxds\right)}, \ t\geq0.
\end{equation}
Da Prato and Zabczyk [-@DaPrato2014] form the space $\mathscr{N}^2_W(0,T)$ of Hilbert-Schmidt operator-valued predictable processes $\Phi(t)$ that satisfy $\|\Phi\|_T<+\infty$ for some $T>0$. This corresponds to our more specialized space $\mathscr{N}_2$ that consists of $L^2(\mathbb{R})$ valued processes $\varphi(x,t)$ such that $\|\varphi\|_t<+\infty$ for all $t\geq0$. In their treatment, $W(t)$ plays a similar role to our generalized process $\mathbf{W}_t$. For $\Phi\in\mathscr{N}^2_W(0,T)$, they denote the stochastic integral for $t\in[0,T]$ by $\Phi\cdot W(t)$. Hence, for $\Phi(t) g(x)=\varphi(x,t)g(x)$ as above, $\mathbf{W}_t(\varphi)=\Phi\cdot W(t)$. The authors then prove the following:

  > __Proposition 4.28__ _Assume that $\Phi_1,\Phi_2\in\mathscr{N}^2_W(0,T)$. Then_
  >
  > $$\mathbb{E}(\Phi_i\cdot W(t))=0, \ \ \mathbb{E}(\|\Phi_i\cdot W(t)\|^2)<+\infty, \ \ \forall \ t\in[0,T].$$
  >
  > __Corollory 4.29__ _Under the same assumptions as Proposition 4.28,_
  >
  > $$\mathbb{C}(\Phi_1\cdot W(t),\Phi_2\cdot W(s))=\mathbb{E}\left(\int_0^{t\wedge s}\mathrm{Tr}[\Phi_2(r)\Phi_1^*(r)]dr\right), \ \ \forall \ t,s\in[0,T].$$

Simplifying these expressions for the multiplication operators described above returns equations (\ref{exp_WN_xi}) and (\ref{cov_WN_xi}) of the main text.

# Simulating the rescaled process \label{numerical}

Here we provide a detailed description of the branching random walk and how we have chosen to rescale it. We focus on the case
\begin{equation}
m(\nu,x)=r-\frac{a}{2}(\theta-x)^2-c\int_\mathbb{R}\nu(x,t)dx.
\end{equation}


## Description of simulation \label{description}

We begin by describing the branching random walk before introducing our scheme to rescale it. Our branching random walk follows closely the description of branching Brownian motion in the main text. However, we replace exponentially distributed lifetimes with deterministic unit time steps for easier implementation. Hence, we restrict time to $t=0,1,2,\dots,$ and so on. Furthermore, we allow individual fitness to depend on both trait value and the state of the entire population. For time $t$ we write $\{\xi_1(t),\dots,\xi_{N(t)}(t)\}$ as the set of trait values across all $N(t)$ individuals alive in the population. Since our simulation follows discrete individuals instead of continuous distributions of trait values, we can write
\begin{equation}
\nu(x,t)=\sum_{i=1}^{N(t)}\delta(x-\xi_i(t)),
\end{equation}
where $\delta(x-\xi_i(t))$ denotes the point mass located at $\xi_i(t)$. For simplicity we assume perfect heritability. At each iteration we draw, for each individual, a random number of offspring from a Negative-Binomial distribution. Recall the Negative-Binomial distribution models the number of failed Bernoulli trials that occur before a given number of successful trials. Denoting $q$ the probability of success for each trial and $s$ the number of successes, the mean and variance is given respectively by
\begin{equation}
\frac{s(1-q)}{q}, \ \frac{s(1-q)}{q^2}.
\end{equation}
Then if we require the $i$th individual to have mean number offspring $\mathscr W(\nu,\xi_i)$ and variance equal to $V$, the parameters of the associated Negative-Binomial distribution become
\begin{equation}
q(\nu,\xi_i)=\frac{\mathscr{W}(\nu,\xi_i)}{V}, \ s(\nu,\xi_i)=\frac{\mathscr{W}^2(\nu,\xi_i)}{V-\mathscr{W}(\nu,\xi_i)}.
\end{equation}
This imposes the restriction $V>\mathscr{W}(\nu,\xi_i)$. For each offpsring produced by the individual with trait value $\xi_i(t)$, we assign indepently drawn trait values normally distributed around $\xi_i(t)$ with variance $\mu$. This summarizes the basic structure of our simulation. To impose selection and density dependent growth rates, we set
\begin{equation}
\mathscr{W}(\nu,\xi_i)=\exp\left(r-\frac{a}{2}(\theta-\xi_i)^2-c\int_\mathbb{R}\nu(x,t)dx\right),
\end{equation}
where the above integral becomes $\int_\mathbb{R}\nu(x,t)dx=\sum_{i=1}^{N(t)}1=N(t)$.

## Rescaling

To rescale the branching random walk by a positive integer $n$, we rescale mutational variance by $\mu\to \mu/n$, time by $t\to t/n$ and the reproductive law by $V\to V$ and
\begin{equation}
\mathscr W(\nu,\xi_i)\to\mathscr W^{(n)}(\nu,\xi_i)=\exp\left(\frac{r}{n}-\frac{a}{2n}(\theta-\xi_i)^2-\frac{c}{n^2}N(t)\right)=\exp\left(\frac{r}{n}-\frac{a}{2n}(\theta-\xi_i)^2-\frac{c}{n}N^{(n)}(t)\right).
\end{equation}
We also replace individual mass with $\frac{1}{n}$ and write rescaled abundance as $N^{(n)}(t)=\frac{1}{n}N(nt)$. When it exists, we denote by $N^{(\infty)}(t)$ the limiting process of $N^{(n)}(t)$. Then
\begin{equation}
\lim_{n\to\infty}n\left(\mathscr W^{(n)}(\nu,\xi_i)-1\right)=r-\frac{a}{2}(\theta-\xi_i)^2-cN^{(\infty)}(t).
\end{equation}
Note that, since the limiting fitness function is bounded above and decreases linearly with respect to $N^{(\infty)}(t)$, it satisfies the hypotheses of Champagnat, Ferrière and Méléard [-@Champagnat2006]. We have implemented this simulation in the programming language Julia. A copy can be found at the url: 
$$\texttt{\url{https://github.com/bobweek/branching.brownian.motion.and.spde}}$$
For the sake of illustration, we simulated the unscaled process ($n=1$) and the rescaled process with $n=5$ and $n=20$ for $50$ units of time. Results are shown in Figure \ref{rescaled} of the main text.

# Derivation of SDE for $\bar x$ and $\sigma^2$ \label{SDE_DERIV}

Picking up from \S\ref{stochastic} of the main text, we have
\begin{equation}
\tilde x(t)=\int_\mathbb{R}x\nu(x,t)dx, \ \ \tilde{\tilde\sigma}^2(t)=\int_\mathbb{R}x^2\nu(x,t)dx
\end{equation}
and
\begin{equation}\label{xtilde}
\tilde x(t)=\tilde x(0)+\int_0^t\int_\mathbb{R}\nu(x,s)m(\nu,x)x+x\sqrt{V\nu(x,s)}\dot W(x,s)dxds,
\end{equation}
\begin{equation}\label{stilde}
\tilde{\tilde\sigma}^2(t)=\tilde{\tilde\sigma}^2(0)+\int_0^t\int_\mathbb{R}\nu(x,s)\left(m(\nu,x)x^2+\mu\right)+x^2\sqrt{V\nu(x,s)}\dot W(x,s)dxds.
\end{equation}


<!--

For $\nu(x,t)$ defined in the main text, $h\in C(\mathbb{R})$ and $t\geq0$ we make the following assumptions:

\begin{equation}\label{L1_nu}
\mathbb{E}\left(\int_\mathbb{R}|h(x)|\nu(x,t)dx\right)<\infty,
\end{equation}
\begin{equation}\label{L2_nu}
\mathbb{E}\left(\int_\mathbb{R}h^2(x)\nu(x,t)dx\right)<\infty,
\end{equation}
\begin{equation}\label{L1_deriv}
\mathbb{E}\left(\int_0^t\int_\mathbb{R}\nu(x,s)\Big|h(x)\Big|\left|m(\nu,x)f(x)+\frac{\mu}{2}\Delta f(x)\right|dxds\right)<\infty, \ \forall f\in C^2_b(\mathbb{R}).
\end{equation}

Put $H(t)=\int_\mathbb{R}h(x)\nu(x,t)dx$. Then, for non-random and non-negative $\nu_1(x,t)$ that is continuous in both arguments and integrable in $x$,
\begin{equation}
\star:=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}[H(t+\varepsilon)-H(t)|\nu(x,t)=\nu_1(x,t)]
\end{equation}
\begin{equation}
=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}\left[\int_\mathbb{R}h(x)\big(\nu(x,t+\varepsilon)-\nu(x,t)\big)dx\Big|\nu(x,t)=\nu_1(x,t)\right]
\end{equation}
\begin{equation}
=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}\left[\int_\mathbb{R}\int_t^{t+\varepsilon}f(x)\left(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)
+\sqrt{V\nu(x,s)}\dot W(x,s)\right)dsdx\Big|\nu(x,t)=\nu_1(x,t)\right].
\end{equation}

By assumption (\ref{L1_deriv}) we can use Fubini's theorem to write, with probability one,

\begin{equation}
\int_\mathbb{R}\int_t^{t+\varepsilon}h(x)\big(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)dsdx=\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\big(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)dxds.
\end{equation}

By assumption (\ref{L2_nu}) we have $\varphi(x,t)=h(x)\sqrt{V\nu(x,t)}$ implies $\varphi\in L^2_c(\mathbb{R}\times[0,\infty))$. Hence, the following is true by definition;

\begin{equation}
\int_\mathbb{R}\int_t^{t+\varepsilon}h(x)\sqrt{V\nu(x,s)}\dot W(x,s)dsdx = 
\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\sqrt{V\nu(x,s)}\dot W(x,s)dxds.
\end{equation}

Hence,

\begin{multline}
\mathbb{E}\left[\int_\mathbb{R}\int_t^{t+\varepsilon}h(x)\sqrt{V\nu(x,s)}\dot W(x,s)dsdx\Big|\nu(x,t)=\nu_1(x,t)\right] \\ =\mathbb{E}\left[\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\sqrt{V\nu(x,s)}\dot W(x,s)dxds\Big|\nu(x,t)=\nu_1(x,t)\right]=0
\end{multline}

and

\begin{equation}
\star=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}\left[\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\left(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)\right)dxds\Big|\nu(x,t)=\nu_1(x,t)\right].
\end{equation}

By assumption (\ref{L1_deriv}) we know that there exists a $\delta>0$ such that for each positive $\varepsilon<\delta$ the following holds almost surely:

\begin{multline}
\left|\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\left(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)\right)dxds\right| \\
\leq\int_t^{t+\delta}\int_\mathbb{R}\Big|h(x)\Big|\left|m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)\right|dxds<\infty.
\end{multline}

Thus, by Lebesgue's dominated convergence theorem, the drift component of the process $H(t)$ can be computed as

\begin{multline}
\star=\mathbb{E}\left[\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\left(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)\right)dxds\Big|\nu(x,t)=\nu_1(x,t)\right]\\
=\int_\mathbb{R}h(x)\big(m(\nu_1,x)\nu_1(x,t)+\frac{\mu}{2}\Delta\nu_1(x,t)\big)dx.
\end{multline}

To find an expression for the diffusion component of $H(t)$ set

\begin{multline}\label{star2}
\star\star:=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{V}[H(t+\varepsilon)-H(t)|\nu(x,t)=\nu_1(x,t)] \\
=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{V}\left[\int_\mathbb{R}h(x)\big(\nu(x,t+h)-\nu(x,t)\big)dx\Big|\nu(x,t)=\nu_1(x,t)\right].
\end{multline}

We can rewrite the integral inside expression (\ref{star2}) as

\begin{equation}
\int_\mathbb{R}\int_t^{t+\varepsilon}h(x)\big(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)+\sqrt{V\nu(x,s)}\dot W(x,s)\big)dsdx.
\end{equation}

We have already found

\begin{multline}
\mathbb{E}\left[\int_\mathbb{R}\int_t^{t+\varepsilon}h(x)\big(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)+\sqrt{V\nu(x,s)}\dot W(x,s)\big)dsdx\Big|\nu(x,t)=\nu_1(x,t)\right] \\
=\mathbb{E}\left[\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\big(m(\nu,x)\nu(x,s)+\frac{\mu}{2}\Delta\nu(x,s)\big)dxds\Big|\nu(x,t)=\nu_1(x,t)\right].
\end{multline}

Then, since $h^2(x)\nu(x,t)$ is integrable by assumption (\ref{L2_nu}), we have

\begin{multline}
\star\star=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}\left[\left(\int_t^{t+\varepsilon}\int_\mathbb{R}h(x)\sqrt{V\nu(x,s)}\dot W(x,s)dsdx\right)^2\Big| \ \nu(x,t)=\nu_1(x,t)\right] \\
=\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\mathbb{E}\left[\int_t^{t+\varepsilon}\int_\mathbb{R}V h^2(x)\nu(x,s)dxds\Big|\nu(x,t)=\nu_1(x,t)\right].
\end{multline}

Thus, for any $\delta\geq\varepsilon\geq0$, we have, with probability one,

\begin{equation}
\int_t^{t+\varepsilon}\int_\mathbb{R}V h^2(x)\nu(x,s)dxds\leq\int_t^{t+\delta}\int_\mathbb{R}V h^2(x)\nu(x,s)dxds.
\end{equation}

We can therefore use Lebesgue's dominated convergence theorem to justify

\begin{equation}
\star\star=\mathbb{E}\left[\lim_{\varepsilon\to0}\frac{1}{\varepsilon}\int_t^{t+\varepsilon}\int_\mathbb{R}V h^2(x)\nu(x,s)dxds\Big|\nu(x,t)=\nu_1(x,t)\right]=\int_\mathbb{R}V h^2(x)\nu_1(x,t)dx.
\end{equation}

Then, using the notation of stochastic differentials, we have

\begin{equation}\label{SDE_deriv}
dH(t)=\left(\int_\mathbb{R}h(x)\left(m(\nu,x)\nu(x,t)+\frac{\mu}{2}\Delta\nu(x,t)\right)dx\right)dt+\sqrt{V\int_\mathbb{R}h^2(x)\nu(x,t)dx} \ dW(t)
\end{equation}

where $W$ is a standard Brownian motion. In the following subsections we employ this formula under the cases $h(x)=x,x^2$ to obtain SDE for the phenotypic mean and variance.

-->

## Derivation for trait mean

We make use of the notation

\begin{equation}
\left\{\begin{matrix}
\|N\|_2 = \sqrt{V\int_\mathbb{R}\nu(x,t)dx}= \sqrt{VN} \\ \\
\|\tilde x\|_2 = \sqrt{V\int_\mathbb{R}x^2\nu(x,t)dx} \\ \\
\langle\tilde x,N\rangle = V\int_\mathbb{R}x\nu(x,t)dx=\bar x VN.
\end{matrix}\right.
\end{equation}

Rewriting formula (\ref{xtilde}) as an SDE provides

\begin{equation}
d\tilde x=\left(\overline{xm}N+\frac{\mu}{2}\int_\mathbb{R}x\Delta\nu(x,t)dx\right)dt+\|\tilde x\|_2d\tilde W_2,
\end{equation}

where,

\begin{equation}
d\tilde{ W}_2=d\hat{\mathbf W}_t(\sqrt{V x^2\nu})=\frac{1}{\|\tilde x\|_2}\int_\mathbb{R}x\sqrt{V\nu(x,t)}\dot W(x,t)dxdt.
\end{equation}

Using It&ocirc;'s quotient rule on $\bar x=\tilde x/N$, we obtain

\begin{equation}
d\bar x = d\left(\frac{\tilde x}{N}\right)=\frac{\tilde x}{N}\left(\frac{d\tilde x}{\tilde x}-\frac{dN}{N}-\frac{d\tilde x}{\tilde x}\frac{dN}{N}+\Big(\frac{dN}{N}\Big)^2\right)
= \frac{d\tilde x}{N}-\bar x\frac{dN}{N}-\frac{d\tilde x}{N}\frac{dN}{N}+\bar x\Big(\frac{dN}{N}\Big)^2.
\end{equation}

From Table \ref{mult} of the main text $d\tilde xdN=\langle\tilde x,N\rangle$ and $dN^2=\|N\|_2^2$. Hence,

\begin{equation}
d\bar x= \overline{xm}dt+\frac{\|\tilde x\|_2}{N}d\tilde W_2-\bar x\left(\bar m dt+\sqrt\frac{V}{N}dW_1\right)-\frac{\langle\tilde x,N\rangle}{N^2}dt+\bar x\frac{\|N\|_2^2}{N^2}dt
\end{equation}

\begin{equation*}
= (\overline{xm}-\bar x\bar m)dt+\frac{\|\tilde x\|_2}{N}d\tilde W_2
-\bar x\sqrt{\frac{V}{N}}d W_1-V\frac{\bar x}{N}dt+V\frac{\bar x}{N}dt
\end{equation*}

\begin{equation*}
=\mathrm{Cov}_t(x,m)
+\frac{\|\tilde x\|_2}{N}d\tilde W_2-\bar x\sqrt{\frac{V}{N}}d W_1.
\end{equation*}

Note that

\begin{equation}
\frac{\|\tilde x\|_2}{N}d\tilde W_2-\bar x\sqrt{\frac{V}{N}}d W_1=\frac{1}{N}\int_\mathbb{R}x\sqrt{V\nu(x,t)}\dot W(x,t)dx-\frac{\bar x}{N}\int_\mathbb{R}\sqrt{V\nu(x,t)}\dot W(x,t)dx
\end{equation}

\begin{equation*}
=\int_\mathbb{R}\frac{x-\bar x}{N}\sqrt{V\nu(x,t)}\dot W(x,t)dx
\end{equation*}

and

\begin{equation}
\mathbb{V}\left(\int_\mathbb{R}\frac{x-\bar x}{N}\sqrt{V\nu(x,t)}\dot W(x,t)dx\right)=\frac{V}{N}\int_\mathbb{R}(x-\bar x)^2p(x,t)dx=V\frac{\sigma^2}{N}.
\end{equation}

Hence, by setting 

\begin{equation}
d W_2=\frac{\int_\mathbb{R}\frac{(x-\bar x)}{N}\sqrt{V\nu(x,t)}\dot W(x,t)dx}{\sqrt{V\sigma^2/N}}
\end{equation}

we can write

\begin{equation}
d\bar x=\mathrm{Cov}_t(x,m)dt
+\sqrt{V\frac{\sigma^2}{N}}d W_2.
\end{equation}

## Derivation for trait variance

We make use of the notation

\begin{equation}
\left\{\begin{matrix}
\|\tilde{\tilde\sigma}^2\|_2 = \sqrt{V\int_\mathbb{R}x^4\nu(x,t)dx} \\ \\
\langle\tilde{\tilde\sigma}^2,N\rangle=V\int_\mathbb{R}x^2\nu(x,t)dx=\overline{x^2}VN.
\end{matrix}\right.
\end{equation}

Applying formula (\ref{stilde}) provides
  
\begin{equation}
d\tilde{\tilde\sigma}^2=\left(\overline{x^2m}N+\mu N\right)dt+\|\tilde{\tilde\sigma}^2\|_2d\tilde W_3
\end{equation}
  
where

\begin{equation}
d\tilde{W}_3=d\hat{\mathbf W}_t(\sqrt{V x^4\nu})
=\frac{1}{\|\tilde{\tilde\sigma}^2\|_2}\int_\mathbb{R}x^2\sqrt{V\nu(x,t)}\dot W(x,t)dx.
\end{equation}

Using It&ocirc;'s quotient rule on $\overline{x^2}=\tilde{\tilde\sigma}^2/N$, we obtain

\begin{equation}
d\overline{x^2} = d\left(\frac{\tilde{\tilde\sigma}^2}{N}\right)=\frac{\tilde{\tilde\sigma}^2}{N}\left(\frac{d\tilde{\tilde\sigma}^2}{\tilde{\tilde\sigma}^2}-\frac{dN}{N}-\frac{d\tilde{\tilde\sigma}^2}{\tilde{\tilde\sigma}^2}\frac{dN}{N}+\Big(\frac{dN}{N}\Big)^2\right)
=\frac{d\tilde{\tilde\sigma}^2}{N}-\overline{x^2}\frac{dN}{N}-\frac{d\tilde{\tilde\sigma}^2}{N}\frac{dN}{N}+\overline{x^2}\left(\frac{dN}{N}\right)^2.
\end{equation}

Table \ref{mult} of the main text implies $d\tilde W_3d W_1=\langle\tilde{\tilde\sigma}^2,N\rangle$ and hence

\begin{multline}
d\overline{x^2}=\left(\overline{x^2m} + \mu \right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3
-\overline{x^2}\left(\bar m dt+\sqrt\frac{V}{N}dW_1\right)-\frac{\langle\tilde{\tilde\sigma}^2,N\rangle}{N^2}dt+\overline{x^2}\frac{\|N\|_2^2}{N^2}dt \\ \\
=\left(\overline{x^2m}-\overline{x^2}\bar mdt + \mu\right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3
-\overline{x^2}\sqrt{\frac{V}{N}}d W_1
-\overline{x^2}\frac{V}{N}dt+\overline{x^2}\frac{V}{N}dt \\ \\
=\left(\mathrm{Cov}_t\Big(x^2,m\Big) + \mu \right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3-\overline{x^2}\sqrt\frac{V}{N}d W_1.
\end{multline}

Setting $F(y,z)=y-z^2$, use It&ocirc;'s formula on $\sigma^2=F(\overline{x^2},\bar x)=\overline{x^2}-\bar x^2$ to obtain:

\begin{multline}
d\sigma^2=d\overline{x^2}-2\bar xd\bar x-(d\bar x)^2
=\left(\mathrm{Cov}_t\Big(x^2,m\Big) + \mu \right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3-\overline{x^2}\sqrt\frac{V}{N}d W_1 \\ \\
-2\bar x\left(\mathrm{Cov}_t(x,m)+\mu dt+\sqrt{\frac{V\sigma^2}{N}}d W_2\right)-\left(\mathrm{Cov}_t(x,m)dt+\mu dt+\sqrt{\frac{V\sigma^2}{N}}d W_2\right)^2 \\ \\
=\left(\mathrm{Cov}_t\Big(x^2-2\bar xx,m\Big) + \mu \right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3-\overline{x^2}\sqrt\frac{V}{N}d W_1-2\bar x\sqrt{\frac{V\sigma^2}{N}}d W_2-\left(\frac{V\sigma^2}{N}\right)dt \\ \\
=\left(\mathrm{Cov}_t\Big(x-\bar x)^2,m\Big) + \mu - \frac{V\sigma^2}{N}\right)dt+\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3-\overline{x^2}\sqrt\frac{V}{N}d W_1-2\bar x\sqrt{\frac{V\sigma^2}{N}}d W_2.
\end{multline}

In light of

\begin{multline}
\frac{\|\tilde{\tilde\sigma}^2\|_2}{N}d\tilde W_3-\overline{x^2}\sqrt\frac{V}{N}d W_1-2\bar x\sqrt{\frac{V\sigma^2}{N}}d W_2=\frac{1}{N}\int_\mathbb{R}\left(x^2-\tilde\sigma^2-2\bar x(x-\bar x)\right)\sqrt{V\nu(x,t)}\dot W(x,t)dx \\
=\frac{1}{N}\int_\mathbb{R}\left((x-\bar x)^2-\sigma^2\right)\sqrt{V\nu(x,t)}\dot W(x,t)dx
\end{multline}

and

\begin{multline}
\frac{1}{N}\int_\mathbb{R}\Big(\left((x-\bar x)^2-\sigma^2\right)\sqrt{V\nu(x,s)}\Big)^2dx
=\frac{V}{N}\left(\int_\mathbb{R}((x-\bar x)^4-2(x-\bar x)^2\sigma^2+\sigma^4)p(x,t)dx\right) \\
=\frac{V}{N}\left(\overline{(x-\bar x)^4}-\sigma^4\right)
\end{multline}

we set

\begin{equation}
d W_3=\frac{\int_\mathbb{R}\left((x-\bar x)^2-\sigma^2\right)\sqrt{V\nu(x,t)}\dot W(x,t)dx}{V\left(\overline{(x-\bar x)^4}-\sigma^4\right)}
\end{equation}

so that

\begin{equation}
d\sigma^2=\mathrm{Cov}_t\Big((x-\bar x)^2,m\Big)dt+\left(\mu-V\frac{\sigma^2}{N}\right)dt+\sqrt{V\frac{\overline{(x-\bar x)^4}-\sigma^4}{N}}d W_3.
\end{equation}

Table \ref{mult} of the main text implies

\begin{equation}
d W_1d W_2=\frac{\int_\mathbb{R}(x-\bar x)\nu(x,t)dx}{\sqrt{N\sigma^2}}dt=0,
\end{equation}

\begin{equation}
d W_1d W_3=\frac{\int_\mathbb{R}\big((x-\bar x)^2-\sigma^2\big)\nu(x,t)dx}{\sqrt{\overline{(x-\bar x)^4}-\sigma^4}}dt=0,
\end{equation}

\begin{equation}
d W_2d W_3=\frac{\int_\mathbb{R}(x-\bar x)\big((x-\bar x)^2-\sigma^2\big)p(x,t)dx}{\sqrt{\sigma^2\big(\overline{(x-\bar x)^4}-\sigma^4\big)}}dt
=\frac{N\overline{(x-\bar x)^3}}{\sqrt{\sigma^2\big(\overline{(x-\bar x)^4}-\sigma^4\big)}}dt.
\end{equation}

In particular, when $p$ is a Gaussian curve $d W_2d W_3=0$.

# Relating fitness of expressed traits to fitness of breeding values \label{fit2fit}

Following \S\ref{inheritance} of the main text, we have $\sigma^2=G+\eta$ and

\begin{equation}
m^*(\rho,g)=\int_\mathbb{R}m(\nu,x)\psi(x,g)dx.
\end{equation}

Hence,

\begin{multline}
\overline{\frac{\partial m^*}{\partial\bar x}}=\int_{\mathbb{R}}\frac{\rho(g,t)}{N(t)}\frac{\partial}{\partial\bar x}\int_\mathbb{R}m(\nu,x)\psi(x,g)dxdg= \\
\int_{\mathbb{R}}\int_\mathbb{R}\frac{\rho(g,t)}{N(t)}\psi(x,g)dg\frac{\partial}{\partial\bar x}m(\nu,x)dx=\int_\mathbb{R}p(x,t)\frac{\partial}{\partial\bar x}m(\nu,x)dx=\overline{\frac{\partial m}{\partial\bar x}}
\end{multline}
and

\begin{multline}
\overline{\frac{\partial m^*}{\partial G}}=\int_{\mathbb{R}}\frac{\rho(g,t)}{N(t)}\frac{\partial}{\partial G}\int_\mathbb{R}m(\nu,x)\psi(x,g)dxdg=\int_{\mathbb{R}}\int_\mathbb{R}\frac{\rho(g,t)}{N(t)}\psi(x,g)dg\frac{\partial m}{\partial G}dx= \\
\int_\mathbb{R}p(x,t)\frac{\partial m}{\partial\sigma^2}\frac{\partial\sigma^2}{\partial G}dx=\overline{\frac{\partial m}{\partial\sigma^2}}.
\end{multline}

# Derivation of diffuse coevolution model \label{diffuse}

In this section we provide a derivation of our model of diffuse coevolution driven by competition. Since most of the work in this derivation has already been completed in Supplementary Material \S\ref{SDE_DERIV}, we focus here on deriving the Malthusian fitness $m$ as a function of trait value $x$. We begin with discrete populations of individuals. In particular, we begin by assuming population size $n_i$ is an integer for each species $i=1,\dots,S$ before passing to the large population size limit.

The reduction in fitness for an individual of species $i$ caused by competition is captured multiplicatively by $0<C_i\leq1$. Biologically this assumes all competitors affect individuals of a given species equally by consuming the same amount of resources. This is a mean-field interaction since any individual that consumes resources has an effect on the fitness of all other individuals competing for the same resources. Denote by $x_{ij}$ the trait value of the $j$-th individual belonging to species $i$. The set of trait values across all individuals in the community at time $t\geq0$ is written $X=\{x_{ij}\}$. We denote by $\mathscr{B}_{ij}$ a function that maps $X$ to the cumulative effect of all competitive interactions on the fitness of the $j$-th individual in species $i$. Since individuals do not compete with themselves the net multiplicative effects on fitness of both interspecific and intraspecific competition on the $j$-th individual in species $i$ can be summarized by

\begin{equation}
\mathscr B_{ij}(X)=C_i^{\sum_{l\neq j}\mathcal{O}_{ii}(x_{ij}-x_{il})+\sum_{k\neq i}\sum_{l=1}^{n_k}\mathcal{O}_{ik}(x_{ij}-x_{kl})},
\end{equation}

where $\mathcal{O}_{ij}$, defined in the main text, measures the overlap in resource use between individuals of species $i$ and $j$ as a function of their niche-centers. Writing $\mathscr W_{ij}(X)$ as the average number of offspring left by the $j$-th individual of species $i$, we have

\begin{equation}
\mathscr W_{ij}(X)=\mathscr{A}_i(x_{ij})\mathscr{B}_{ij}(X),
\end{equation}

where $\mathscr{A}_i(x)=\int_\mathbb{R}e_i(\zeta)u_i(\zeta,x)d\zeta$ accounts for abiotic selection and $e_i$ has been defined in the main text.

We now turn to a diffusion limit. Since we have more than one population, we take the diffusion limit for each population one at a time starting with population $1$. We write $\pmb n=(n_1,\dots,n_S)$. Following Méléard and Roelly [-@Mlard1993; -@meleard1992interacting] we rescale generation time and individual mass to $\frac{1}{n_1}$ and mean of the reproductive law to

\begin{equation}
\mathscr W_{1j}^{(\pmb n)}(X)=\mathscr{A}_1(x_{1j})^{1/n_1}\exp\left(\frac{\ln C_1}{n_1^2}\sum_{l\neq j}\mathcal{O}_{11}(x_{1j}-x_{1l})+\frac{\ln C_1}{n_1}\sum_{k\neq i}\frac{1}{n_k}\sum_{l=1}^{n_k}\mathcal{O}_{1k}(x_{1j}-x_{kl})\right).
\end{equation}

For large $n_1$, we have the approximation

\begin{equation}
\mathscr W_{1j}^{(\pmb n)}(X)\approx\mathscr{A}_1(x_{1j})^{1/n_1}\left(1+\frac{\ln C_1}{n_1^2}\sum_{l\neq j}\mathcal{O}_{11}(x_{1j}-x_{1l})+\frac{\ln C_1}{n_1}\sum_{k\neq 1}\frac{1}{n_k}\sum_{l=1}^{n_k}\mathcal{O}_{1k}(x_{1j}-x_{kl})\right).
\end{equation}
Hence
\begin{equation}
\lim_{n_1\to\infty}n_1\left(\mathscr W_{1j}^{(\pmb n)}(X)-1\right)=\ln\mathscr{A}_1(x_{1j})+\left(\int_\mathbb{R}\mathcal{O}_{11}(x_{1j}-y)\nu_1(y,t)dy+\sum_{k\neq 1}\frac{1}{n_k}\sum_{l=1}^{n_k}\mathcal{O}_{1k}(x_{1j}-x_{kl})\right)\ln C_1.
\end{equation}

We write $\lim_{\pmb n\to\pmb\infty}$ for the iterated limit $\lim_{n_S\to\infty}\cdots\lim_{n_1\to\infty}$ and, assuming  $\nu_i(\cdot,t)\in C_1^+(\mathbb{R})$ for $i=1,\dots,S$ and $t\in[0,\infty)$, we set $\pmb\nu=(\nu_1,\dots,\nu_S)$. Then, for any $\pmb\nu$, the the diffusion limits for the remaining populations provides the Malthusian parameter for individuals in species $i$ with trait value $x_{1j}$ as

\begin{equation}
m_1(\pmb\nu,x_{1j}):=\lim_{\pmb n\to\pmb\infty}n_1\left(\mathscr W_{1j}^{(\pmb n)}(X)-1\right)=\ln\mathscr{A}_1(x)+\left(\sum_{k=1}^S\int_\mathbb{R}\mathcal{O}_{1k}(x_{1j}-y)\nu_k(y,t)dy\right)\ln C_1.
\end{equation}

We compute the average niche overlap of an individual in species $i$ with nich location $x$ across all individuals in species $j$ as

\begin{equation}
\bar{\mathcal{O}}_{ij}(x,t)=\frac{\int_\mathbb{R}\mathcal{O}_{ij}(x-y)\nu_j(y,t)dy}{\int_\mathbb{R}\nu_j(y,t)dy}.
\end{equation}

We now assume the resource utilization curves $u_i(\zeta)$ and phenotypic densities $\nu_i(x,t)$ are Gaussian curves for $i=1,\dots,S$. In this case $\bar{\mathcal{O}}_{ij}(x,t)$ simplifies to

\begin{equation}
\bar{\mathcal{O}}_{ij}(x,t)=\frac{\int_\mathbb{R}\mathcal O_{ij}(x-y)\nu_j(y,t)dy}{\int_\mathbb{R}\nu_j(y,t)dy}
=\frac{U_iU_j}{\sqrt{2\pi(w_i+w_j+{\sigma_j}^2(t))}}\exp\left(-\frac{(x-\bar x_j(t))^2}{2(w_i+w_j+{\sigma_j}^2(t))}\right).
\end{equation}

Setting

\begin{subequations}
\begin{align}
\sigma_i^2(t) = & \ G_i(t)+\eta_i, \\
R_i = & \ \ln \left(\frac{Q_iU_i}{\sqrt{1+A_iw_i}}\right), \\
a_i = & \ \frac{A_i}{1+A_iw_i}, \\
\tilde b_{ij}(t) = & \ \frac{1}{w_i+w_j+\sigma_j^2(t)}, \\
c_i = & -\ln C_i,
\end{align}
\end{subequations}

we get

\begin{equation}\label{malth_fit}
m_i(\pmb\nu,x)=R_i-\frac{a_i}{2}(x-\theta_i)^2-c_i\sum_{j=1}^SN_j(t)U_iU_j\sqrt{\frac{\tilde b_{ij}(t)}{2\pi}}e^{-\frac{\tilde b_{ij}(t)}{2}(x-\bar x_j(t))^2}.
\end{equation}

Hence, our fitness function is bounded above and decreases linearly with total abundance, as required by the assumptions of Champagnat, Ferrière and Méléard [-@Champagnat2006]. For the remainder of the derivation we suppress notation indicating dependency on $\pmb\nu$, $x$ and $t$. From (\ref{malth_fit}) we calculate

\begin{equation}
\frac{\partial m_i}{\partial\bar x_i}=c_iN_iU_i^2\tilde b_{ii}(x-\bar x_i)\sqrt{\frac{\tilde b_{ii}}{2\pi}}e^{-\frac{\tilde b_{ii}}{2}(x-\bar x_i)^2}
\end{equation}

\begin{multline}
\frac{\partial m_i}{\partial G_i}=\frac{c_iN_iU_i^2}{2}\left(\frac{(x-\bar x_i)^2-G_i-\eta_i-2w_i}{(G_i+\eta_i+2w_i)^2}\right)\sqrt{\frac{\tilde b_{ii}}{2\pi}}e^{-\frac{\tilde b_{ii}}{2}(x-\bar x_i)^2} \\
=\frac{c_iN_iU_i^2{\tilde b_{ii}}^2}{2}\left((x-\bar x_i)^2-{\sigma_i}^2-2w_i\right)\sqrt{\frac{\tilde b_{ii}}{2\pi}}e^{-\frac{\tilde b_{ii}}{2}(x-\bar x_i)^2}.
\end{multline}

Note that

\begin{multline}
\sqrt\frac{\tilde b_{ii}}{2\pi}\exp\left(-\frac{\tilde b_{ii}}{2}(x-\bar x_i)^2\right)\sqrt\frac{1}{2\pi{\sigma_i}^2}\exp\left(-\frac{(x-\bar x_i)^2}{2{\sigma_i}^2}\right) \\
=\sqrt\frac{1}{2\pi({\sigma_i}^2+1/\tilde b_{ii})}\sqrt\frac{{\sigma_i}^2+1/\tilde b_{ii}}{2\pi{\sigma_i}^2/\tilde b_{ii}}\exp\left(-\frac{{\sigma_i}^2+1/\tilde b_{ii}}{2{\sigma_i}^2/\tilde b_{ii}}(x-\bar x_i)^2\right) \\
=\sqrt{\frac{1}{4\pi({\sigma_i}^2+w_i)}}\sqrt\frac{2({\sigma_i}^2+w_i)}{2\pi{\sigma_i}^2({\sigma_i}^2+2w_i)}\exp\left(-\frac{{\sigma_i}^2({\sigma_i}^2+2w_i)}{4({\sigma_i}^2+w_i)}(x-\bar x_i)^2\right).
\end{multline}

Hence,

\begin{equation}
\overline{\frac{\partial m_i}{\partial\bar x_i}}=0,
\end{equation}

\begin{multline}
\overline{\frac{\partial m_i}{\partial G_i}}=\frac{c_iN_iU_i^2}{2({\sigma_i}^2+2w_i)^2}\left(\frac{({\sigma_i}^2+2w_i){\sigma_i}^2}{2(w_i+{\sigma_i}^2)}-{\sigma_i}^2-2w_i\right)\sqrt{\frac{b_{ii}}{2\pi}} \\
=\frac{c_iN_iU_i^2}{2({\sigma_i}^2+2w_i)}\left(\frac{{\sigma_i}^2}{2({\sigma_i}^2+w_i)}-1\right)\sqrt{\frac{b_{ii}}{2\pi}}=-\frac{c_iN_iU_i^2b_{ii}}{2}\sqrt{\frac{b_{ii}}{2\pi}},
\end{multline}

where

\begin{equation}
b_{ij}=\frac{1}{w_i+w_j+{\sigma_i}^2+{\sigma_j}^2}.
\end{equation}

The average fitness for species $i$ is

\begin{equation}
\bar m_i=R_i-\frac{a_i}{2}\Big((\bar x_i-\theta_i)^2+G_i+\eta_i\Big) - c_i\sum_{j=1}^SN_jU_iU_j\sqrt{\frac{b_{ij}}{2\pi}}e^{-\frac{b_{ij}}{2}(\bar x_i-\bar x_j)^2}.
\end{equation}

Thus,

\begin{equation}
\frac{\partial\bar m_i}{\partial\bar x_i}=a_i(\theta_i-\bar x_i)-c_i\sum_jN_jU_iU_jb_{ij}(\bar x_j-\bar x_i)\sqrt{\frac{b_{ij}}{2\pi}}e^{-\frac{b_{ij}}{2}(\bar x_i-\bar x_j)^2},
\end{equation}

\begin{equation}
\frac{\partial\bar m_i}{\partial G_i}=-\frac{a_i}{2}+\frac{c_i}{2}\sum_{j=1}^SN_jU_iU_jb_{ij}\left(1-b_{ij}(\bar x_i-\bar x_j)^2\right)\sqrt{\frac{b_{ij}}{2\pi}}e^{-\frac{b_{ij}}{2}(\bar x_i-\bar x_j)^2}.
\end{equation}

In particular

\begin{equation}
\frac{\partial\bar m_i}{\partial G_i}-\overline{\frac{\partial m_i}{\partial G_i}}=-\frac{a_i}{2}+\frac{c_i}{2}\left(N_iU_i^2b_{ii}\sqrt\frac{b_{ii}}{2\pi}+\sum_{j=1}^SN_jU_iU_jb_{ij}\left(1-b_{ij}(\bar x_i-\bar x_j)^2\right)\sqrt{\frac{b_{ij}}{2\pi}}e^{-\frac{b_{ij}}{2}(\bar x_i-\bar x_j)^2}\right).
\end{equation}

Applying equations (\ref{N}), (\ref{xbarfinal}) and (\ref{Gfinal}) of the main text recovers system (\ref{comm_dynamics}) of the main text.

# The relation between competition coefficients and selection \label{coef_grad_moms}

## Derivation of analytical approximations

Just as with most caclulations in this work, the derivations are straightforward applications of Gaussian products. That is, if

\begin{equation}
f_1(x)=\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(\mu_1-x)^2}{2\sigma_1^2}\right), \ f_2(x)=\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(\mu_2-x)^2}{2\sigma_2^2}\right),
\end{equation}

then

\begin{equation}
f_1(x)f_2(x)=\frac{1}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}\exp\left(-\frac{(\mu_1-\mu_2)^2}{2(\sigma_1^2+\sigma_2^2)}\right)\frac{1}{\sqrt{2\pi\tilde\sigma^2}}\exp\left(-\frac{(\tilde\mu-x)^2}{2\tilde\sigma^2}\right),
\end{equation}

where

\begin{equation}
\tilde\mu=\frac{\sigma_2^2\mu_1+\sigma_1^2\mu_2}{\sigma_1^2+\sigma_2^2}, \ \tilde\sigma^2=\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}.
\end{equation}

### Caclulating $\mathrm{Cov}_{f_{\bar X}}(\alpha,\gamma)$

Recalling

\begin{equation}
\alpha(\bar x_i,\bar x_j)=\frac{c}{\bar r}\sqrt\frac{b}{2\pi}\exp\left(-\frac{b}{2}\big(\bar x_i-\bar x_j\big)^2\right),
\end{equation}

\begin{equation}
\gamma(\bar x_i,\bar x_j)=cNb\left(1-b\big(\bar x_i-\bar x_j\big)^2\right) \sqrt\frac{b}{2\pi}\exp\left(-\frac{b}{2}\big(\bar x_i-\bar x_j\big)^2\right),
\end{equation}

we have

\begin{multline}
\bar\alpha=
\int_\mathbb{R}\int_\mathbb{R}\alpha(\bar x_i,\bar x_j)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\frac{c}{\bar r}\int_\mathbb{R}\frac{1}{\sqrt{2\pi(b^{-1}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(b^{-1}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j
=\frac{c/\bar r}{\sqrt{2\pi(b^{-1}+2V_{\bar X})}},
\end{multline}

\begin{multline}
\bar\gamma=
\int_\mathbb{R}\int_\mathbb{R}\gamma(\bar x_i,\bar x_j)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
= cNb\int_\mathbb{R}\left\{1-\left[\left(\frac{\bar{\bar x}+bV_{\bar X}\bar x_j}{1+bV_{\bar X}}-\bar x_j\right)^2+\frac{V_{\bar X}}{1+bV_{\bar X}}\right]\right\}\frac{1}{\sqrt{2\pi(b^{-1}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(b^{-1}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j \\
= cNb\int_\mathbb{R}\left\{1-\left[\left(\frac{\bar{\bar x}-\bar x_j}{1+bV_{\bar X}}\right)^2+\frac{V_{\bar X}}{1+bV_{\bar X}}\right]\right\}\frac{1}{\sqrt{2\pi(b^{-1}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(b^{-1}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j \\
= cNb\left(1-\frac{(1+bV_{\bar X})V_{\bar X}}{1+2bV_{\bar X}}\frac{1}{(1+bV_{\bar X})^2}-\frac{V_{\bar X}}{1+bV_{\bar X}}\right)\frac{1}{\sqrt{2\pi(b^{-1}+2V_{\bar X})}} \\
= cNb\left[1-\left(\frac{1}{1+2bV_{\bar X}}+1\right)\frac{V_{\bar X}}{1+bV_{\bar X}}\right]\frac{1}{\sqrt{2\pi(b^{-1}+2V_{\bar X})}} \\
= cNb\left(1-\frac{2V_{\bar X}}{1+2bV_{\bar X}}\right)\sqrt\frac{b}{2\pi(1+2bV_{\bar X})},
\end{multline}

\begin{multline}
\mathrm{Var}_{f_{\bar X}}(\alpha)=
\int_\mathbb{R}\int_\mathbb{R}\big(\bar\alpha-\alpha(\bar x_i,\bar x_j)\big)^2f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\frac{c^2}{\bar r^2}\left(\sqrt\frac{b}{4\pi}\int_\mathbb{R}\int_\mathbb{R}\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j-\frac{1}{2\pi(b^{-1}+2V_{\bar X})}\right) \\
=\frac{c^2}{\bar r^2}\left(\sqrt\frac{b}{4\pi}\int_\mathbb{R}\sqrt\frac{1}{2\pi(\frac{1}{2b}+V_{\bar X})}\exp\left(-b(\bar{\bar x}-\bar x_j)^2\right)f_{\bar X}(\bar x_j)d\bar x_j-\frac{1}{2\pi(b^{-1}+2V_{\bar X})}\right) \\
=\frac{c^2}{\bar r^2}\left(\sqrt\frac{b}{4\pi}\sqrt\frac{1}{2\pi(\frac{1}{2b}+2V_{\bar X})}-\frac{1}{2\pi(b^{-1}+2V_{\bar X})}\right)=\frac{c^2b}{2\pi\bar r^2}\left(\frac{1}{\sqrt{1+4bV_{\bar X}}}-\frac{1}{1+2bV_{\bar X}}\right),
\end{multline}

\begin{multline}
\mathrm{Cov}_{f_{\bar X}}(\alpha,\gamma)=
\int_\mathbb{R}\int_\mathbb{R}\big(\bar\alpha-\alpha(\bar x_i,\bar x_j)\big)\big(\bar\gamma-\gamma(\bar x_i,\bar x_j)\big)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\frac{c^2Nb}{2\bar r}\sqrt\frac{b}{\pi}\int_\mathbb{R}\int_\mathbb{R}\big(1-b(\bar x_i-\bar x_j)^2\big)\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j-\bar\alpha\bar\gamma \\
=\frac{c^2Nb}{2\bar r}\sqrt\frac{b}{\pi}\frac{1-2bV_{\bar X}}{\sqrt{2\pi((2b)^{-1}+2V_{\bar X})}}-\frac{c^2Nb}{\bar r}\frac{1-2bV_{\bar X}}{2\pi(b^{-1}+2V_{\bar X})} \\
=\frac{c^2b^2N}{2\pi\bar r}(1-2bV_{\bar X})\left(\frac{1}{\sqrt{1+4bV_{\bar X}}}-\frac{1}{1+2bV_{\bar X}}\right).
\end{multline}

### Caclulating $\mathrm{Cov}_{f_{\bar X}}(\alpha,|\beta|)$

To calculate moments of $|\beta|$ we note that, as a random variable, $|\beta|$ takes a folded normal distribution. Setting $\Phi(x)$ equal to the cumulative density function of the standard normal distribution and using the properties of the folded normal distribution, we find

\begin{equation}
\overline{|\beta|}=\sqrt\frac{2\mathrm{Var}_{f_{\bar X}}(\beta)}{\pi}\exp\left(-\frac{\bar\beta^2}{2\mathrm{Var}_{f_{\bar X}}(\beta)}\right)-\bar\beta\left[1-2\Phi\left(\frac{\bar\beta}{\mathrm{Var}_{f_{\bar X}}(\beta)}\right)\right]
\end{equation}

\begin{equation}
\mathrm{Var}_{f_{\bar X}}(|\beta|)=\bar\beta^2+\mathrm{Var}_{f_{\bar X}}(\beta)-\overline{|\beta|}^2.
\end{equation}

Recall that

\begin{equation}
\beta(\bar x_i,\bar x_j)=cNb\big(\bar x_i-\bar x_j\big)\sqrt\frac{b}{2\pi}\exp\left(-\frac{b}{2}\big(\bar x_i-\bar x_j\big)^2\right)
\end{equation}

and hence

\begin{multline}
\bar\beta=
\int_\mathbb{R}\int_\mathbb{R}\beta(\bar x_i,\bar x_j)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
= cNb\int_\mathbb{R}(\bar{\bar x}-\bar x_j)\frac{1}{\sqrt{2\pi(b^{-1}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(b^{-1}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j=0,
\end{multline}

\begin{multline}
\mathrm{Var}_{f_{\bar X}}(\beta)=
\int_\mathbb{R}\int_\mathbb{R}\big(\bar\beta-\beta(\bar x_i,\bar x_j)\big)^2f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\int_\mathbb{R}\int_\mathbb{R}c^2N^2b^2(\bar x_i-\bar x_j)^2\frac{b}{2\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\sqrt{\frac{b}{4\pi}}c^2N^2b^2\int_\mathbb{R}\left[\left(\frac{\bar{\bar x}+2bV_{\bar X}\bar x_j}{1+2bV_{\bar X}}-\bar x_j\right)^2+\frac{V_{\bar X}}{1+2bV_{\bar X}}\right]\frac{\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(\frac{1}{2b}+V_{\bar X})}\right)}{\sqrt{2\pi(\frac{1}{2b}+V_{\bar X})}}f_{\bar X}(\bar x_j)d\bar x_j \\
=\sqrt{\frac{b}{4\pi}}c^2N^2b^2\int_\mathbb{R}\left[\frac{(\bar{\bar x}-\bar x_j)^2}{(1+2bV_{\bar X})^2}+\frac{V_{\bar X}}{1+2bV_{\bar X}}\right]\frac{\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(\frac{1}{2b}+V_{\bar X})}\right)}{\sqrt{2\pi(\frac{1}{2b}+V_{\bar X})}}f_{\bar X}(\bar x_j)d\bar x_j \\
=\sqrt{\frac{b}{4\pi}}c^2N^2b^2\left[\frac{(1+2bV_{\bar X})V_{\bar X}}{1+4bV_{\bar X}}\frac{1}{(1+2bV_{\bar X})^2}+\frac{V_{\bar X}}{1+2bV_{\bar X}}\right]\frac{1}{\sqrt{2\pi(\frac{1}{2b}+2V_{\bar X})}} \\
=\frac{b}{\pi}\frac{c^2N^2b^2}{\sqrt{1+4bV_{\bar X}}}\frac{V_{\bar X}}{1+2bV_{\bar X}}\left(\frac{1}{1+4bV_{\bar X}}+1\right)=\frac{2c^2N^2b^3V_{\bar X}}{\pi(1+4bV_{\bar X})^{3/2}}.
\end{multline}

Thus, using properties of the folded normal distribution, we find

\begin{equation}
\overline{|\beta|}=\sqrt{\frac{2}{\pi}}\frac{cNb^{3/2}}{(1+4bV_{\bar X})^{3/4}}\sqrt\frac{2V_{\bar X}}{\pi}=\frac{2}{\pi}\frac{cNb^{3/2}}{(1+4bV_{\bar X})^{3/4}}\sqrt{V_{\bar X}},
\end{equation}

\begin{equation}
\mathrm{Var}_{f_{\bar X}}(|\beta|)=\frac{c^2N^2b^3}{(1+4bV_{\bar X})^{3/2}}\frac{2V_{\bar X}}{\pi}\left(1-\frac{2}{\pi}\right).
\end{equation}

We also calculate

\begin{multline}
\mathrm{Cov}_{f_{\bar X}}(\alpha,\beta)=
\int_\mathbb{R}\int_\mathbb{R}\big(\bar\alpha-\alpha(\bar x_i,\bar x_j)\big)\big(\bar\beta-\beta(\bar x_i,\bar x_j)\big)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\frac{c^2Nb}{2\bar r}\sqrt\frac{b}{\pi}\int_\mathbb{R}\int_\mathbb{R}(\bar x_i-\bar x_j)\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j=0.
\end{multline}

In attempt to calculate $\mathrm{Cov}_{f_{\bar X}}(\alpha,|\beta|)$ we find

\begin{multline}
\mathrm{Cov}f_{\bar X}(\alpha,|\beta|)=\int_\mathbb{R}\int_\mathbb{R}\alpha(\bar x_i,\bar x_j)|\beta(\bar x_i,\bar x_j)|f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j-\bar\alpha\overline{|\beta|} \\
=\int_\mathbb{R}\int_\mathbb{R}\frac{c}{\bar r}\sqrt\frac{b}{2\pi}\exp\left(-\frac{b}{2}(\bar x_i-\bar x_j)^2\right)cNb|\bar x_i-\bar x_j|\sqrt\frac{b}{2\pi}\exp\left(-\frac{b}{2}(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j-\bar\alpha\overline{|\beta|} \\
=\frac{c^2Nb}{\bar r}\sqrt\frac{b}{4\pi}\int_\mathbb{R}\int_\mathbb{R}|\bar x_i-\bar x_j|\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j-\bar\alpha\overline{|\beta|}.
\end{multline}

Just as we used the folded normal to find $\overline{|\beta|}$ and $\mathrm{Var}_{f_{\bar X}}(|\beta|)$, we can calculate $\mathrm{Cov}_{f_{\bar X}}(\alpha,|\beta|)$ by considering

\begin{equation}
\int_\mathbb{R}\int_\mathbb{R}(\bar x_i-\bar x_j)\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j=0
\end{equation}

and

\begin{multline}
\int_\mathbb{R}\int_\mathbb{R}(\bar x_i-\bar x_j)^2\frac{b}{\pi}\exp\left(-2b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\sqrt\frac{2b}{\pi}\int_\mathbb{R}\int_\mathbb{R}(\bar x_i-\bar x_j)^2\frac{1}{\sqrt{2\pi\frac{1}{4b}}}\exp\left(-\frac{(\bar x_i-\bar x_j)^2}{2\frac{1}{4b}}\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j \\
=\sqrt\frac{2b}{\pi}\int_\mathbb{R}\left[\left(\frac{\bar{\bar x}+4bV_{\bar X}\bar x_j}{1+4bV_{\bar X}}-\bar x_j\right)^2+\frac{V_{\bar X}}{1+4bV_{\bar X}}\right]\frac{1}{\sqrt{2\pi(\frac{1}{4b}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(\frac{1}{4b}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j \\
=\sqrt\frac{2b}{\pi}\int_\mathbb{R}\left[\left(\frac{\bar{\bar x}-\bar x_j}{1+4bV_{\bar X}}\right)^2+\frac{V_{\bar X}}{1+4bV_{\bar X}}\right]\frac{1}{\sqrt{2\pi(\frac{1}{4b}+V_{\bar X})}}\exp\left(-\frac{(\bar{\bar x}-\bar x_j)^2}{2(\frac{1}{4b}+V_{\bar X})}\right)f_{\bar X}(\bar x_j)d\bar x_j \\
=\sqrt\frac{2b}{\pi}\left[\frac{(1+4bV_{\bar X})V_{\bar X}}{1+8bV_{\bar X}}\frac{1}{(1+4bV_{\bar X})^2}+\frac{V_{\bar X}}{1+4bV_{\bar X}}\right]\frac{1}{\sqrt{2\pi(\frac{1}{4b}+2V_{\bar X})}} \\
=\sqrt\frac{2b}{\pi}\frac{2V_{\bar X}}{1+8bV_{\bar X}}\sqrt{\frac{4b}{2\pi(1+8bV_{\bar X})}}=\frac{b}{\pi}\frac{4V_{\bar X}}{(1+8bV_{\bar X})^{3/2}}.
\end{multline}

Hence

\begin{equation}
\int_\mathbb{R}\int_\mathbb{R}|\bar x_i-\bar x_j|\sqrt\frac{b}{\pi}\exp\left(-b(\bar x_i-\bar x_j)^2\right)f_{\bar X}(\bar x_i)f_{\bar X}(\bar x_j)d\bar x_id\bar x_j=\sqrt\frac{2}{\pi}\sqrt{\frac{b}{\pi}\frac{4V_{\bar X}}{(1+8bV_{\bar X})^{3/2}}}=\frac{2}{\pi}\frac{\sqrt{2bV_{\bar X}}}{(1+8bV_{\bar X})^{3/4}}
\end{equation}

and

\begin{multline}
\mathrm{Cov}_{f_{\bar X}}(\alpha,|\beta|)=\frac{c^2Nb}{\bar r}\sqrt\frac{b}{4\pi}\frac{2}{\pi}\frac{\sqrt{2bV_{\bar X}}}{(1+8bV_{\bar X})^{3/4}}-\bar\alpha\overline{|\beta|} \\
=\frac{2c^2Nb^2}{\pi\bar r(1+8bV_{\bar X})^{3/4}}\sqrt\frac{V_{\bar X}}{2\pi}-\frac{c}{\bar r}\sqrt\frac{b}{2\pi(1+2bV_{\bar X})}\frac{2}{\pi}\frac{cNb^{3/2}}{(1+4bV_{\bar X})^{3/4}}\sqrt{V_{\bar X}} \\
=\frac{2c^2Nb^2}{\pi\bar r(1+8bV_{\bar X})^{3/4}}\sqrt\frac{V_{\bar X}}{2\pi}-\frac{2c^2Nb^2}{\pi\bar r(1+4bV_{\bar X})^{3/4}}\sqrt\frac{V_{\bar X}}{2\pi(1+2bV_{\bar X})} \\
=\frac{2c^2Nb^2}{\pi\bar r}\sqrt\frac{V_{\bar X}}{2\pi}\left(\frac{1}{(1+8bV_{\bar X})^{3/4}}-\frac{1}{(1+4bV_{\bar X})^{3/4}(1+2bV_{\bar X})^{1/2}}\right).
\end{multline}

### Starting the caclulation of $\mathrm{Cov}_{f_{\bar X}}(\alpha,\mathfrak{C})$ \label{caC}

We have

\begin{equation}
\mathfrak{C}(\bar x_i,\bar x_j)=c^2N^2b^2\Big(|\bar x_i-\bar x_j|+\big|1-b(\bar x_i-\bar x_j)^2\big|\Big)^2\exp\left(-\frac{b}{2}(\bar x_i-\bar x_j)^2\right).
\end{equation}

Note that the random variable $\delta=\bar x_i-\bar x_j$ is a mean zero Gaussian random variable with variance $2V_{\bar X}$. We write the probability density function of $\delta$ as $f_{\Delta}(\delta)$. Substituting in $\delta$, we can write

\begin{multline}
\mathfrak{C}(\delta,0)=c^2N^2b^2\Big(|\delta|+\big|1-b\delta^2\big|\Big)^2\exp\left(-\frac{b}{2}\delta^2\right) \\
=c^2N^2b^2\Big(\delta^2+2\big||\delta|-b|\delta|^3\big|+\big(1-b\delta^2\big)^2\Big)\exp\left(-\frac{b}{2}\delta^2\right).
\end{multline}

From this expression, we see properties of the folded normal distribution can be used to calculate several components of the intregal $\mathrm{Cov}_{f_{\bar X}}(\alpha,\mathfrak{C})$, but a major technical challenge lies in calculating

\begin{equation}
\int_{\mathbb{R}}\big||\delta|-b|\delta|^3\big|\exp\left(-\frac{b}{2}\delta^2\right)f_{\Delta}(\delta)d\delta.
\end{equation}

Instead of overcoming this challenge to find an analytical form of $\mathrm{Cov}_{f_{\bar X}}(\alpha,\mathfrak{C})$ we turn to a numerical approach outlined in the following section.

## Numerical estimates for heterogeneous $N$ and $G$

To verify our analytical estimates for the covariance of selection gradients and competition coefficients, equations (\ref{cov_eqns}) of the main text, and for approximating the correlation of competition coefficients and a metric of pairwise coevolution, Figure \ref{cov_fig} of the main text, we developed a numerical approach by repeatedly simulating our system (\ref{comm_dynamics}) of the main text across a set of randomly drawn model parameters. In particular, we ran each numerical integration with $S=100$ species for $T_1=1000.0$ units of time. We then continued to integrate for $T_2=1000.0$ units of time and calculated the covariances between competition coefficients and the quantities $|\beta|$, $\gamma$ and $\mathfrak{C}$ for each of the last $T_2$ time steps. We assume the temporal average of these covariances across the last $T_2$ units of time approximates the expectation at equilibrium. We repeated this approach for randomly drawn $a$ and $c$ until our sample size reached $1000$. We drew values for $a$ and $c$ from condition log-normal distributions. For $a$, we set the log-normal parameters equal to $\mu=-10$ and $\sigma=6$ and conditioned on $a<0.01$ to prevent extinction. For $c$ we set the log-normal parameters equal to $\mu=-4$ and $\sigma=4$ and conditioned on $c<1\times 10^{-4}$ to prevent very large interspecific phenotypic variation and extinction. When extinction occurs, we aborted the simulation and restarted with newly drawn parameters. Results for $\mathrm{Corr}_{f_{\bar X}}(\alpha,|\beta|)$ and $\mathrm{Corr}_{f_{\bar X}}(\alpha,\gamma)$ are displayed in SM Figure \ref{num_corr}.

```{r, echo=F, fig.align='center', out.width='100%', fig.cap='\\label{num_corr} Numerical estimate for the correlations of selection gradients and competition coefficients.'}
knitr::include_graphics('/home/bb/Gits/branching.brownian.motion.and.spde/.A White Noise Approach To Evolutionary Ecology/corrs.png')
```

# References